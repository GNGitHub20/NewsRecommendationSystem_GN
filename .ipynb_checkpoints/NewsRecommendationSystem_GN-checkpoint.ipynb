{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-c7fa16b5f600>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-c7fa16b5f600>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    iPrint, an upcoming Indian media house that offers media and information services to the people, has decided to begin providing a more personalised experience to its customers. The company’s business extends across a wide range of media, including news and information services on sports, weather, education, health, research, stocks and healthcare.\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Problem Statement\n",
    "\n",
    "iPrint, an upcoming Indian media house that offers media and information services to the people, has decided to begin providing a more personalised experience to its customers. The company’s business extends across a wide range of media, including news and information services on sports, weather, education, health, research, stocks and healthcare.\n",
    "\n",
    "iPrint has been managing its customer base by only recommending the most popular and similar news articles to its users. However, the recommended news articles are often not relevant to the majority of the users. And the company is not able to recommend any new content to its customers, and gradually, the company sthas started to lose such users, resulting in immense revenue loss.\n",
    "\n",
    "In order to stay healthy in the market, iPrint needs to stay updated with time an technology advancements.\n",
    "\n",
    "\n",
    "# Suggestion\n",
    "\n",
    "A probable solution for the company to compete with other competitors is to solve the issue of revenue leakage by personalising users' tastes and introducing new content to its users at the start of the day on the home page of the application.\n",
    "\n",
    "\n",
    "# Solution\n",
    "\n",
    "##### Building a NEWS RECOMMENDATION SYSTEM that does below:\n",
    "\n",
    "--> Recommend new top 10 relevant articles to a user when he/she visits the app at the start of the day\n",
    "\n",
    "--> Recommend top 10 similar news articles that match the ones clicked by the user.\n",
    "\n",
    "The articles that are written in the English language must be considered.\n",
    "\n",
    "## 1. Data Pre-Processing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', 50000)\n",
    "pd.set_option('display.max_columns', 50000)\n",
    "\n",
    "consumer_ds = pd.read_csv('data/consumer_transanctions.csv', low_memory=False)\n",
    "\n",
    "platform_ds = pd.read_csv('data/platform_content.csv')\n",
    "\n",
    "consumer_ds.head()\n",
    "\n",
    "platform_ds.head()\n",
    "\n",
    "#### Increase column width to see Keywords values properly\n",
    "\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "consumer_ds.head(2)\n",
    "\n",
    "platform_ds.head(2)\n",
    "\n",
    "consumer_ds.info()\n",
    "\n",
    "consumer_ds.shape\n",
    "\n",
    "##### Imputing 'Rating' values on the feature 'interaction_type' - numeric\n",
    "\n",
    "consumer_ds.interaction_type.nunique()\n",
    "\n",
    "consumer_ds['interaction_type'].unique()\n",
    "\n",
    "consumer_ds['interaction_type'].value_counts()\n",
    "\n",
    "# Defining the function for Imputing\n",
    "def ratings(x):\n",
    "    if x == 'content_followed':\n",
    "        return 5\n",
    "    elif x == 'content_commented_on':\n",
    "        return 4\n",
    "    elif x == 'content_saved':\n",
    "        return 3\n",
    "    elif x == 'content_liked':\n",
    "        return 2\n",
    "    elif x == 'content_watched':\n",
    "        return 1\n",
    "\n",
    "consumer_ds['Ratings'] = consumer_ds['interaction_type'].apply(lambda x: ratings(x))\n",
    "\n",
    "consumer_ds.head()\n",
    "\n",
    "consumer_ds['Ratings'].value_counts()\n",
    "\n",
    "consumer_ds.describe()\n",
    "\n",
    "\n",
    "\n",
    "#### Platform Dataset Analysis on Keywords Column\n",
    "\n",
    "platform_ds.shape\n",
    "\n",
    "platform_ds.language.nunique()\n",
    "\n",
    "platform_ds['language'].unique()\n",
    "\n",
    "platform_ds1 = platform_ds[platform_ds['language']=='en']\n",
    "\n",
    "platform_ds1['language'].unique()\n",
    "\n",
    "platform_ds1.head(2)\n",
    "\n",
    "platform_ds1.shape\n",
    "\n",
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "plt.hist(consumer_ds.interaction_type)\n",
    "plt.xlabel('Interaction Type from Consumer DF', fontsize=18)\n",
    "plt.ylabel('Distribution', fontsize=18)\n",
    "plt.xticks(rotation=75, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "##### As visible from above histogram, the most comment interaction type for users on the app is 'content_watched', following by some 'content_liked'\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2, 1, 1)\n",
    "ax.set_yscale('log')\n",
    "plt.hist(platform_ds.item_type)\n",
    "plt.xlabel('Item Types', fontsize=18)\n",
    "plt.ylabel('Distribution', fontsize=18)\n",
    "plt.xticks(rotation=75, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "##### As visible from above histogram, the most comment item type is 'HTML'\n",
    "\n",
    "plt.hist(platform_ds.interaction_type)\n",
    "plt.xlabel('Interaction Type from Platform DF', fontsize=18)\n",
    "plt.ylabel('Distribution', fontsize=18)\n",
    "plt.xticks(rotation=75, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "##### As visible from above histogram, the most comment interaction type from (platform dataset) is 'content_present'\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2, 1, 1)\n",
    "ax.set_yscale('log')\n",
    "plt.hist(platform_ds.language)\n",
    "plt.xlabel('Interaction Type', fontsize=18)\n",
    "plt.ylabel('Distribution', fontsize=18)\n",
    "plt.xticks(rotation=75, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "##### As visible from above histogram, the most comment language of users is 'en', that is, English followed by 'pt' for Portuguese\n",
    "\n",
    "\n",
    "### Some Bar Graph Plots\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2, 1, 1)\n",
    "ax.set_yscale('log')\n",
    "country=consumer_ds.country\n",
    "country.value_counts().plot(kind='bar', color='red')\n",
    "plt.xlabel('Consumer Countries', fontsize=14)\n",
    "plt.ylabel('Number of news', fontsize=14)\n",
    "plt.xticks(rotation=80, fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "consumer_ds['device_name'] = consumer_ds.consumer_device_info.str.split().str.get(0)\n",
    "\n",
    "consumer_ds.head()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2, 1, 1)\n",
    "ax.set_yscale('log')\n",
    "device_n=consumer_ds.device_name\n",
    "device_n.value_counts().plot(kind='bar', color='green')\n",
    "plt.xlabel('Device Name Info', fontsize=14)\n",
    "plt.ylabel('Number of news', fontsize=14)\n",
    "plt.xticks(rotation=80, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2, 1, 1)\n",
    "ax.set_yscale('log')\n",
    "prod_country=platform_ds1.producer_country\n",
    "prod_country.value_counts().plot(kind='bar', color='orange')\n",
    "plt.xlabel('Producer Countries', fontsize=14)\n",
    "plt.ylabel('Number of news', fontsize=14)\n",
    "plt.xticks(rotation=80, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Interaction Type --> Ratings Calculation\n",
    "\n",
    "interact_ds1 = consumer_ds['interaction_type'].value_counts()*100/consumer_ds.shape[0]\n",
    "\n",
    "interact_ds = pd.DataFrame(interact_ds1)\n",
    "\n",
    "interact_ds.reset_index(inplace=True)\n",
    "\n",
    "interact_ds.columns = ['interaction_type', 'value']\n",
    "interact_ds\n",
    "\n",
    "# Rating Value\n",
    "\n",
    "interact_ds['rating'] = 100/interact_ds['value']\n",
    "interact_ds\n",
    "\n",
    "user_rating = pd.merge(consumer_ds, interact_ds, on='interaction_type', how='left')\n",
    "\n",
    "user_rating = user_rating[['consumer_id', 'item_id', 'rating']]\n",
    "\n",
    "user_rating.drop_duplicates(inplace=True)\n",
    "user_rating.shape\n",
    "\n",
    "user_rating.head(3)\n",
    "\n",
    "user_rating['news_id'] = user_rating.groupby(['item_id']).ngroup()\n",
    "\n",
    "user_rating['user_id'] = user_rating.groupby(['consumer_id']).ngroup()\n",
    "\n",
    "user_rating.head(3)\n",
    "\n",
    "ratings_ds = user_rating[['user_id','news_id','rating']]\n",
    "\n",
    "ratings_ds.head(3)\n",
    "\n",
    "\n",
    "\n",
    "# 3. Collaborative Filtering: User Based Recommendations\n",
    "\n",
    "consumer_ds.head()\n",
    "\n",
    "consumer_ds.item_id.nunique()\n",
    "\n",
    "# Test and Train split of the dataset.\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(consumer_ds, test_size=0.30, random_state=31)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "# Pivot the train ratings' dataset into matrix format in which columns are item IDs and the rows are user IDs.\n",
    "df_pivot = train.pivot_table(\n",
    "    index='consumer_id',\n",
    "    columns='item_id',\n",
    "    values='Ratings'\n",
    ").fillna(0)\n",
    "\n",
    "df_pivot.head()\n",
    "\n",
    "df_pivot.shape\n",
    "\n",
    "# Copy the train dataset into dummy_train\n",
    "dummy_train = train.copy()\n",
    "\n",
    "dummy_train.head(2)\n",
    "\n",
    "# The movies not rated by user is marked as 1 for prediction.\n",
    "dummy_train['Ratings'] = dummy_train['Ratings'].apply(lambda x: 0 if x>=1 else 1)\n",
    "\n",
    "# Convert the dummy train dataset into matrix format.\n",
    "dummy_train = dummy_train.pivot_table(\n",
    "    index='consumer_id',\n",
    "    columns='item_id',\n",
    "    values='Ratings'\n",
    ").fillna(1)\n",
    "\n",
    "dummy_train.head()\n",
    "\n",
    "\n",
    "\n",
    "# User Similarity Matrix\n",
    "\n",
    "## Using Cosine Similarity\n",
    "\n",
    "df_pivot.index.nunique()\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Creating the User Similarity Matrix using pairwise_distance function.\n",
    "user_correlation = 1 - pairwise_distances(df_pivot, metric='cosine')\n",
    "user_correlation[np.isnan(user_correlation)] = 0\n",
    "print(user_correlation)\n",
    "\n",
    "user_correlation.shape\n",
    "\n",
    "## Using adjusted Cosine\n",
    "\n",
    "### Here, we are not removing the NaN values and calculating the mean only for the news rated by the user\n",
    "\n",
    "# Create a user-movie matrix.\n",
    "df_pivot = train.pivot_table(\n",
    "    index='consumer_id',\n",
    "    columns='item_id',\n",
    "    values='Ratings'\n",
    ")\n",
    "\n",
    "df_pivot.head()\n",
    "\n",
    "mean = np.nanmean(df_pivot, axis=1)\n",
    "df_subtracted = (df_pivot.T-mean).T\n",
    "\n",
    "df_subtracted.head()\n",
    "\n",
    "\n",
    "\n",
    "### Finding cosine similarity\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Creating the User Similarity Matrix using pairwise_distance function.\n",
    "user_correlation = 1 - pairwise_distances(df_subtracted.fillna(0), metric='cosine')\n",
    "user_correlation[np.isnan(user_correlation)] = 0\n",
    "print(user_correlation)\n",
    "\n",
    "user_correlation.shape\n",
    "\n",
    "\n",
    "\n",
    "## Prediction - User User\n",
    "\n",
    "\n",
    "user_correlation[user_correlation<0]=0\n",
    "user_correlation\n",
    "\n",
    "user_predicted_ratings = np.dot(user_correlation, df_pivot.fillna(0))\n",
    "user_predicted_ratings\n",
    "\n",
    "user_predicted_ratings.shape\n",
    "\n",
    "user_predicted_ratings\n",
    "\n",
    "user_final_rating = np.multiply(user_predicted_ratings,dummy_train)\n",
    "user_final_rating.head()\n",
    "\n",
    "### Finding the top 10 news recommendation for the user\n",
    "\n",
    "# Take the user ID as input\n",
    "\n",
    "# Taking 1st userID from above list\n",
    "\n",
    "user_input = int(input(\"Enter your user name\"))\n",
    "print(user_input)\n",
    "\n",
    "user_final_rating.head(7)\n",
    "\n",
    "d = user_final_rating.loc[user_input].sort_values(ascending=False)[0:10]\n",
    "d\n",
    "\n",
    "d2 = consumer_ds[['item_id', 'consumer_id']]\n",
    "\n",
    "item_user_pair = pd.merge(d, d2, on = 'item_id', how='left')\n",
    "\n",
    "item_user_pair\n",
    "\n",
    "d3 = pd.merge(d,consumer_ds,left_on='item_id',right_on='item_id', how = 'left')\n",
    "d3\n",
    "\n",
    "# Evaluation - User User\n",
    "\n",
    "# Find out the common users of test and train dataset.\n",
    "common = test[test.consumer_id.isin(train.consumer_id)]\n",
    "common.shape\n",
    "\n",
    "common.head()\n",
    "\n",
    "# convert into the user-movie matrix.\n",
    "common_user_based_matrix = common.pivot_table(index='consumer_id', columns='item_id', values='Ratings')\n",
    "\n",
    "common_user_based_matrix.head()\n",
    "\n",
    "# Convert the user_correlation matrix into dataframe.\n",
    "user_correlation_df = pd.DataFrame(user_correlation)\n",
    "\n",
    "user_correlation_df.head()\n",
    "\n",
    "df_subtracted.head(3)\n",
    "\n",
    "user_correlation_df['consumer_id'] = df_subtracted.index\n",
    "\n",
    "user_correlation_df.set_index('consumer_id',inplace=True)\n",
    "user_correlation_df.head()\n",
    "\n",
    "common.head(3)\n",
    "\n",
    "list_name = common.consumer_id.tolist()\n",
    "\n",
    "user_correlation_df.columns = df_subtracted.index.tolist()\n",
    "\n",
    "\n",
    "user_correlation_df_1 =  user_correlation_df[user_correlation_df.index.isin(list_name)]\n",
    "\n",
    "user_correlation_df_1.shape\n",
    "\n",
    "user_correlation_df_2 = user_correlation_df_1.T[user_correlation_df_1.T.index.isin(list_name)]\n",
    "\n",
    "user_correlation_df_3 = user_correlation_df_2.T\n",
    "\n",
    "user_correlation_df_3.head()\n",
    "\n",
    "user_correlation_df_3.shape\n",
    "\n",
    "user_correlation_df_3[user_correlation_df_3<0]=0\n",
    "\n",
    "common_user_predicted_ratings = np.dot(user_correlation_df_3, common_user_based_matrix.fillna(0))\n",
    "common_user_predicted_ratings\n",
    "\n",
    "dummy_test = common.copy()\n",
    "\n",
    "dummy_test['Ratings'] = dummy_test['Ratings'].apply(lambda x: 1 if x>=1 else 0)\n",
    "\n",
    "dummy_test = dummy_test.pivot_table(index='consumer_id', columns='item_id', values='Ratings').fillna(0)\n",
    "\n",
    "dummy_test.shape\n",
    "\n",
    "common_user_based_matrix.head()\n",
    "\n",
    "dummy_test.head(3)\n",
    "\n",
    "common_user_predicted_ratings = np.multiply(common_user_predicted_ratings,dummy_test)\n",
    "\n",
    "common_user_predicted_ratings.head()\n",
    "\n",
    "# Model Evaluation\n",
    "\n",
    "### RMSE\n",
    "\n",
    "Calculating the RMSE for only the news rated by users. For RMSE, normalising the rating to (1,5) range.\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import *\n",
    "\n",
    "X  = common_user_predicted_ratings.copy()\n",
    "X = X[X>0]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "print(scaler.fit(X))\n",
    "y = (scaler.transform(X))\n",
    "\n",
    "print(y)\n",
    "\n",
    "common_ = common.pivot_table(index='consumer_id', columns='item_id', values='Ratings')\n",
    "\n",
    "# Finding total non-NaN value\n",
    "total_non_nan = np.count_nonzero(~np.isnan(y))\n",
    "\n",
    "rmse = (sum(sum((common_ - y )**2))/total_non_nan)**0.5\n",
    "print(rmse)\n",
    "\n",
    "### Precision@k\n",
    "\n",
    "user_index=20\n",
    "\n",
    "ratings_ds[ratings_ds['user_id']==20]\n",
    "\n",
    "user_set = ratings_ds[ratings_ds['user_id']==user_index].sort_values(by='rating',ascending=False)['news_id'].tolist()\n",
    "\n",
    "user_set\n",
    "\n",
    "len(user_set)\n",
    "\n",
    "predict_ds = pd.DataFrame(user_predicted_ratings)\n",
    "\n",
    "user_predicted_set = predict_ds.iloc[user_index].sort_values(ascending=False)[:10].index.tolist()\n",
    "\n",
    "len(user_predicted_set)\n",
    "\n",
    "user_predicted_set\n",
    "\n",
    "precision_at_10 = len(list(set(user_set) & set(user_predicted_set)))/10\n",
    "\n",
    "precision_at_10\n",
    "\n",
    "# Using Item similarity\n",
    "\n",
    "## Item Based Similarity\n",
    "\n",
    "df_pivot = train.pivot_table(\n",
    "    index='consumer_id',\n",
    "    columns='item_id',\n",
    "    values='Ratings'\n",
    ").T\n",
    "\n",
    "df_pivot.head()\n",
    "\n",
    "mean = np.nanmean(df_pivot, axis=1)\n",
    "df_subtracted = (df_pivot.T-mean).T\n",
    "\n",
    "df_subtracted.head()\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Item Similarity Matrix\n",
    "item_correlation = 1 - pairwise_distances(df_subtracted.fillna(0), metric='cosine')\n",
    "item_correlation[np.isnan(item_correlation)] = 0\n",
    "print(item_correlation)\n",
    "\n",
    "item_correlation.shape\n",
    "\n",
    "item_correlation[item_correlation<0]=0\n",
    "item_correlation\n",
    "\n",
    "## 4. Prediction - Item Item\n",
    "\n",
    "\n",
    "item_predicted_ratings = np.dot((df_pivot.fillna(0).T),item_correlation)\n",
    "item_predicted_ratings\n",
    "\n",
    "item_predicted_ratings.shape\n",
    "\n",
    "dummy_train.shape\n",
    "\n",
    "### Filtering the rating only for the news not rated by the user for recommendation\n",
    "\n",
    "item_final_rating = np.multiply(item_predicted_ratings,dummy_train)\n",
    "item_final_rating.head()\n",
    "\n",
    "### Finding the top 10 recommendation for the *user*\n",
    "\n",
    "# Take the user ID as input\n",
    "\n",
    "# Taking 2nd user ID from above\n",
    "\n",
    "user_input = int(input(\"Enter your user name\"))\n",
    "print(user_input)\n",
    "\n",
    "# Recommending the Top 10 products to the user.\n",
    "d = item_final_rating.loc[user_input].sort_values(ascending=False)[0:10]\n",
    "d\n",
    "\n",
    "d1 = pd.merge(d,consumer_ds,left_on='item_id',right_on='item_id',how = 'left')\n",
    "d1\n",
    "\n",
    "train_new = pd.merge(train,consumer_ds,left_on='item_id',right_on='item_id',how='left')\n",
    "\n",
    "train_new[train_new.consumer_id_x == -9223121837663643404]\n",
    "\n",
    "# Evaluation - Item Item\n",
    "\n",
    "test.columns\n",
    "\n",
    "common =  test[test.item_id.isin(train.item_id)]\n",
    "common.shape\n",
    "\n",
    "common.head()\n",
    "\n",
    "common_item_based_matrix = common.pivot_table(index='consumer_id', columns='item_id', values='Ratings').T\n",
    "\n",
    "common_item_based_matrix.shape\n",
    "\n",
    "item_correlation_df = pd.DataFrame(item_correlation)\n",
    "\n",
    "item_correlation_df.head(3)\n",
    "\n",
    "item_correlation_df['item_id'] = df_subtracted.index\n",
    "item_correlation_df.set_index('item_id',inplace=True)\n",
    "item_correlation_df.head()\n",
    "\n",
    "list_name = common.item_id.tolist()\n",
    "\n",
    "item_correlation_df.columns = df_subtracted.index.tolist()\n",
    "\n",
    "item_correlation_df_1 =  item_correlation_df[item_correlation_df.index.isin(list_name)]\n",
    "\n",
    "item_correlation_df_2 = item_correlation_df_1.T[item_correlation_df_1.T.index.isin(list_name)]\n",
    "\n",
    "item_correlation_df_3 = item_correlation_df_2.T\n",
    "\n",
    "item_correlation_df_3.head()\n",
    "\n",
    "item_correlation_df_3[item_correlation_df_3<0]=0\n",
    "\n",
    "common_item_predicted_ratings = np.dot(item_correlation_df_3, common_item_based_matrix.fillna(0))\n",
    "common_item_predicted_ratings\n",
    "\n",
    "common_item_predicted_ratings.shape\n",
    "\n",
    "dummy_test = common.copy()\n",
    "\n",
    "dummy_test['Ratings'] = dummy_test['Ratings'].apply(lambda x: 1 if x>=1 else 0)\n",
    "\n",
    "dummy_test = dummy_test.pivot_table(index='consumer_id', columns='item_id', values='Ratings').T.fillna(0)\n",
    "\n",
    "common_item_predicted_ratings = np.multiply(common_item_predicted_ratings,dummy_test)\n",
    "\n",
    "common_ = common.pivot_table(index='consumer_id', columns='item_id', values='Ratings').T\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import *\n",
    "\n",
    "X  = common_item_predicted_ratings.copy()\n",
    "X = X[X>0]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(1, 10))\n",
    "print(scaler.fit(X))\n",
    "y = (scaler.transform(X))\n",
    "\n",
    "print(y)\n",
    "\n",
    "# Finding total non-nan value\n",
    "total_non_nan = np.count_nonzero(~np.isnan(y))\n",
    "\n",
    "rmse = (sum(sum((common_ - y )**2))/total_non_nan)**0.5\n",
    "print(rmse)\n",
    "\n",
    "## Filtering News from Recommendation which are already seen by the user\n",
    "\n",
    "ds_user = ratings_ds[ratings_ds['user_id']==17]\n",
    "\n",
    "ds_user.head(3)\n",
    "\n",
    "ds_user.shape\n",
    "\n",
    "ratings_ds.head(3)\n",
    "\n",
    "\n",
    "\n",
    "prediction_ds = pd.DataFrame(user_predicted_ratings)\n",
    "\n",
    "prediction_ds.iloc[17].sort_values(ascending=False)[:10]\n",
    "\n",
    "user_rating_final = user_rating[['item_id', 'news_id']]\n",
    "\n",
    "user_rating_final = user_rating_final.drop_duplicates()\n",
    "\n",
    "user_rating_final.head(2)\n",
    "\n",
    "recommended_news_df = pd.DataFrame(prediction_ds.iloc[17].sort_values(ascending=False))\n",
    "\n",
    "recommended_news_df.reset_index(inplace=True)\n",
    "\n",
    "recommended_news_df.head()\n",
    "\n",
    "platform_ds3 = platform_ds[platform_ds['language']=='en']\n",
    "\n",
    "news_title = platform_ds3[['item_id', 'title']]\n",
    "\n",
    "news_title = platform_ds3[['item_id','title']]\n",
    "\n",
    "news_title.head()\n",
    "\n",
    "recommended_news_df.columns = ['news_id', 'score']\n",
    "\n",
    "recommended_news_df.head(2)\n",
    "\n",
    "recommended_merged = pd.merge(recommended_news_df, user_rating, how='left', on='news_id')\n",
    "\n",
    "recommended_merged.head(2)\n",
    "\n",
    "news_output = pd.merge(recommended_merged,news_title,how='left', on='item_id')\n",
    "\n",
    "news_output = news_output[['news_id', 'score', 'item_id', 'title']]\n",
    "\n",
    "news_output.head(2)\n",
    "\n",
    "merged_filter = pd.merge(news_output, ds_user, on='news_id', how='left')\n",
    "\n",
    "merged_filter.shape\n",
    "\n",
    "merged_filter.head(3)\n",
    "\n",
    "merged_filter = merged_filter.drop(merged_filter[merged_filter['rating']>0].index)\n",
    "\n",
    "merged_filter['title'][:10]\n",
    "\n",
    "\n",
    "\n",
    "# 5. Content-based Filtering\n",
    "\n",
    "platform_ds1['text_description'].str.len()\n",
    "\n",
    "#### Average keywords string length\n",
    "\n",
    "np.mean(platform_ds1['text_description'].str.len())\n",
    "\n",
    "#### Number of Text Description\n",
    "\n",
    "number_of_text_description = []\n",
    "for keywords in platform_ds1['text_description']:\n",
    "    n_keywords = len(keywords.split(','))\n",
    "    number_of_text_description.append(n_keywords)\n",
    "\n",
    "number_of_text_description\n",
    "\n",
    "sum(number_of_text_description)\n",
    "\n",
    "# Plotting\n",
    "\n",
    "plt.hist(number_of_text_description)\n",
    "plt.xlabel('Distribution of Number of Words in News Articles Text Description column')\n",
    "plt.ylabel('Number of News')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "##### Selected DF\n",
    "\n",
    "platform_ds1 = platform_ds1[['title', 'text_description']]\n",
    "\n",
    "platform_ds1.head(2)\n",
    "\n",
    "# Data Pre-Processing\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def make_lower_case(text_description):\n",
    "    return text_description.lower()\n",
    "\n",
    "# Define function for removing stop words\n",
    "def remove_stop_words(text_description):\n",
    "    text_description = text_description.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text_description = [w for w in text_description if not w in stops]\n",
    "    texts = [w for w in text_description if w.isalpha()]\n",
    "    texts = \" \".join(texts)\n",
    "    return texts\n",
    "\n",
    "# Define function for removing punctuation\n",
    "def remove_punctuation(text_description):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_description = tokenizer.tokenize(text_description)\n",
    "    text_description = \" \".join(text_description)\n",
    "    return text_description\n",
    "\n",
    "# Define function for removing the html tags\n",
    "def remove_html(text_description):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text_description)\n",
    "\n",
    "\n",
    "# Applying the above functions in column 'text_description' and storing as a new column named 'cleaned_desc'\n",
    "platform_ds1['cleaned_desc'] = platform_ds1['text_description'].apply(func = make_lower_case)\n",
    "platform_ds1['cleaned_desc'] = platform_ds1.cleaned_desc.apply(func = remove_stop_words)\n",
    "platform_ds1['cleaned_desc'] = platform_ds1.cleaned_desc.apply(func = remove_punctuation)\n",
    "platform_ds1['cleaned_desc'] = platform_ds1.cleaned_desc.apply(func = remove_html)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "tf = TfidfVectorizer(analyzer='word',\n",
    "                     stop_words='english',\n",
    "                     max_df=0.8,\n",
    "                     min_df=0.0,\n",
    "                     use_idf=True,\n",
    "                     ngram_range=(1,3))\n",
    "tfidf_matrix = tf.fit_transform(platform_ds1['cleaned_desc'])\n",
    "\n",
    "platform_ds1.head(2)\n",
    "\n",
    "platform_ds1['cleaned_desc'][:2]\n",
    "\n",
    "platform_ds1 = platform_ds1.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "\n",
    "platform_ds1['cleaned_desc'][:2]\n",
    "\n",
    "platform_ds1.shape\n",
    "\n",
    "### Create numpy array from text_description column\n",
    "\n",
    "keywords_array = platform_ds1['cleaned_desc'].to_numpy()\n",
    "\n",
    "keywords_array\n",
    "\n",
    "type(keywords_array[0])\n",
    "\n",
    "### Use splitting to generate words from keywords_array\n",
    "\n",
    "words_list = []\n",
    "for keyword in keywords_array:\n",
    "    splitted_words = keyword.lower().split(' ')\n",
    "    words_list.append(splitted_words)\n",
    "\n",
    "words_list\n",
    "\n",
    "len(words_list), len(words_list[0]), len(words_list[1]), len(words_list[10])\n",
    "\n",
    "### Create Dictionary\n",
    "\n",
    "# Install Gensim\n",
    "\n",
    "! pip install gensim\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "create a dictionary of words from words list\n",
    "\n",
    "dictionary = Dictionary(words_list)\n",
    "\n",
    "dictionary\n",
    "\n",
    "len(dictionary)\n",
    "\n",
    "#### Total number of words in the words_list\n",
    "\n",
    "number_words = 0\n",
    "for word in words_list:\n",
    "    number_words = number_words + len(word)\n",
    "\n",
    "number_words\n",
    "\n",
    "dictionary.get(0), dictionary.get(1), dictionary.get(20), dictionary.get(1000)\n",
    "\n",
    "## Generate Bag of Words\n",
    "\n",
    "words_list[0]\n",
    "\n",
    "#### doc2bow = Document to BOW[Bag of word]\n",
    "\n",
    "bow = dictionary.doc2bow(words_list[0])\n",
    "\n",
    "bow\n",
    "\n",
    "len(words_list[0]), len(bow)\n",
    "\n",
    "### Generate Corpus by creating BOW of each document\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in words_list]\n",
    "\n",
    "#print(corpus)\n",
    "\n",
    "len(corpus), len(corpus[0]), len(corpus[1]), len(corpus[20])\n",
    "\n",
    "len(words_list), len(words_list[0])\n",
    "\n",
    "### TF-IDF Model\n",
    "\n",
    "#### TF = Term frequency -> count/no.\n",
    "\n",
    "#### IDF = Inverse doc freq. -> log(N/n)\n",
    "\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create tfidf model of the corpus\n",
    "\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "tfidf\n",
    "\n",
    "print(tfidf[corpus[0]])\n",
    "\n",
    "## Generate Similarity Matrix\n",
    "\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "sims = MatrixSimilarity(tfidf[corpus], num_features=len(dictionary))\n",
    "\n",
    "print(sims)\n",
    "\n",
    "sims[corpus[0]]\n",
    "\n",
    "len(sims[corpus[0]])\n",
    "\n",
    "## Generate Recommendation\n",
    "\n",
    "news_title = \"IEEE to Talk Blockchain at Cloud Computing Oxford-Con - CoinDesk\"\n",
    "\n",
    "# number of recommedations\n",
    "\n",
    "n=10\n",
    "\n",
    "platform_ds2 = platform_ds1.loc[platform_ds1.title==news_title]\n",
    "\n",
    "platform_ds2\n",
    "\n",
    "#### Generate words_list by splitting the keywords column\n",
    "\n",
    "words_list = platform_ds2['cleaned_desc'].iloc[0].split(' ')\n",
    "\n",
    "words_list\n",
    "\n",
    "#### set the query_doc to the words_list\n",
    "\n",
    "query_doc = words_list\n",
    "\n",
    "query_doc\n",
    "\n",
    "#### Get the Bag Of Words\n",
    "\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "\n",
    "query_doc_bow\n",
    "\n",
    "len(query_doc_bow), len(query_doc)\n",
    "\n",
    "### Generate TF-IDF values for the query_doc_bow\n",
    "\n",
    "query_doc_tfidf = tfidf[query_doc_bow]\n",
    "\n",
    "query_doc_tfidf\n",
    "\n",
    "\n",
    "\n",
    "### Get Similarity Score using Similarity Matrix\n",
    "\n",
    "similarity_array = sims[query_doc_tfidf]\n",
    "\n",
    "similarity_array\n",
    "\n",
    "len(similarity_array)\n",
    "\n",
    "### Create a Series to Visualize Similarity Score along with News Title\n",
    "\n",
    "similarity_series = pd.Series(similarity_array.tolist(), index=platform_ds1.title.values)\n",
    "\n",
    "similarity_series.head\n",
    "\n",
    "### Sort the series to get Top Recommended News\n",
    "\n",
    "recommended_news = similarity_series.sort_values(ascending=False)[1:n+1]\n",
    "\n",
    "recommended_news.head\n",
    "\n",
    "### Corpus for 15th index\n",
    "\n",
    "corpus[15]\n",
    "\n",
    "corpus[platform_ds2.index.values.tolist()[0]]\n",
    "\n",
    "#### TF-IDF Values\n",
    "\n",
    "tfidf[corpus[platform_ds2.index.values.tolist()[0]]]\n",
    "\n",
    "#### Sorted TF-IDF values\n",
    "\n",
    "sorted_tfidf_weights = sorted(tfidf[corpus[platform_ds1.index.values.tolist()[0]]], key=lambda w: w[1], reverse=True)\n",
    "\n",
    "sorted_tfidf_weights\n",
    "\n",
    "dictionary.get(3000)\n",
    "\n",
    "dictionary.get(5000)\n",
    "\n",
    "print('Top words associated with this news by tf-idf values are: ')\n",
    "for term_id, weight in sorted_tfidf_weights[:10]:\n",
    "    print(\" '%s' %.5f\" %(dictionary.get(term_id), weight))\n",
    "\n",
    "platform_ds2.columns\n",
    "\n",
    "# Creating a Modular code by creating a function for Query News Input\n",
    "\n",
    "def news_recommendation(news_title, number_of_hits):\n",
    "    platform_ds2 = platform_ds1.loc[platform_ds1.title==news_title] # get the news title row\n",
    "\n",
    "    keywords = platform_ds2['cleaned_desc'].iloc[0].split(' ') # get the keywords as a Series (platform_ds2['cleaned_desc'])\n",
    "\n",
    "    query_doc = keywords #set the query_doc to the list of keywords\n",
    "\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc) # get a bag of words from the query_doc\n",
    "\n",
    "    query_doc_tfidf = tfidf[query_doc_bow] # convert the regular bag of words model to a tf-idf model\n",
    "\n",
    "    similarity_array = sims[query_doc_tfidf] # get the array of similarity values between our news and every other news.\n",
    "\n",
    "    similarity_series = pd.Series(similarity_array.tolist(), index=platform_ds1.title.values) # convert to a Series\n",
    "\n",
    "    recommended_news = similarity_series.sort_values(ascending=False)[1:number_of_hits+1]\n",
    "\n",
    "    # get the top matching results, i.e. most similar news title\n",
    "    # start from index 1 because every item is most similar to itself\n",
    "\n",
    "    return recommended_news\n",
    "\n",
    "type(recommended_news)\n",
    "\n",
    "news_recommendation(\"French Senate Will Debate on Bitcoin Regulation\", 10)\n",
    "\n",
    "news_recommendation('Fooling The Machine', 10)\n",
    "\n",
    "# 6. ALS Model - Alternating Least Square\n",
    "\n",
    "## Building Recommendation system using ALS on Consumer Dataset\n",
    "\n",
    "consumer_ds.head(2)\n",
    "\n",
    "consumer_ds.shape\n",
    "\n",
    "consumer_ds['consumer_id'].nunique()\n",
    "\n",
    "consumer_ds['item_id'].nunique()\n",
    "\n",
    "\n",
    "\n",
    "## Creating Sparse User-Item Matrix\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "alpha = 40\n",
    "\n",
    "consumer_ds.shape\n",
    "\n",
    "consumer_ds.dropna(inplace=True)\n",
    "\n",
    "consumer_ds = consumer_ds.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "\n",
    "consumer_ds.shape\n",
    "\n",
    "#consumer_ds['consumer_id_abs'] = consumer_ds['consumer_id'].apply(lambda x: abs(x))\n",
    "\n",
    "#consumer_ds['item_id_abs'] = consumer_ds['item_id'].apply(lambda x: abs(x))\n",
    "\n",
    "consumer_ds.head(2)\n",
    "\n",
    "type(consumer_ds)\n",
    "\n",
    "#sparse_user_item = csr_matrix( ([alpha]*consumer_ds.shape[0], (consumer_ds['consumer_id_abs'], consumer_ds['item_id_abs']) ))\n",
    "\n",
    "users_items_pivot_matrix_df = consumer_ds.pivot_table(index='consumer_id',\n",
    "                                                columns='item_id',\n",
    "                                                values='Ratings').fillna(0)\n",
    "\n",
    "users_items_pivot_matrix_df.head(10)\n",
    "\n",
    "users_ids = list(users_items_pivot_matrix_df.index)\n",
    "users_ids[:10]\n",
    "\n",
    "users_items_pivot_sparse_matrix = csr_matrix(users_items_pivot_matrix_df)\n",
    "users_items_pivot_sparse_matrix\n",
    "\n",
    "Shape 1895x2982 sparse matrix\n",
    "\n",
    "csr_user_array = users_items_pivot_sparse_matrix.toarray()\n",
    "\n",
    "csr_user_array\n",
    "\n",
    "len(csr_user_array), len(csr_user_array[0]), csr_user_array[1][1]\n",
    "\n",
    "max(csr_user_array[1])\n",
    "\n",
    "\n",
    "\n",
    "### csr matrix only stores where value is 40 [non-zero]. (Compressed Sparse Row)\n",
    "\n",
    "print(users_items_pivot_sparse_matrix)\n",
    "\n",
    "users_items_pivot_sparse_matrix = users_items_pivot_sparse_matrix.T.tocsr()\n",
    "\n",
    "users_items_pivot_sparse_matrix\n",
    "\n",
    "csr_item_array = users_items_pivot_sparse_matrix.toarray()\n",
    "\n",
    "csr_item_array\n",
    "\n",
    "len(csr_item_array), len(csr_item_array[0]), csr_item_array[1][1]\n",
    "\n",
    "print(users_items_pivot_sparse_matrix)\n",
    "\n",
    "\n",
    "\n",
    "#! pip install implicit\n",
    "\n",
    "import implicit\n",
    "from implicit.evaluation import train_test_split\n",
    "\n",
    "train, test = train_test_split(users_items_pivot_sparse_matrix, train_percentage=0.8)\n",
    "\n",
    "train\n",
    "\n",
    "test\n",
    "\n",
    "model = implicit.als.AlternatingLeastSquares(factors=100, regularization=0.1, iterations=20, calculate_training_loss=False)\n",
    "\n",
    "model\n",
    "\n",
    "# Train the Model\n",
    "\n",
    "model.fit(train)\n",
    "\n",
    "user_id = 100\n",
    "\n",
    "consumer_ds['consumer_id'].loc[100]\n",
    "\n",
    "model.recommend(user_id, users_items_pivot_sparse_matrix)\n",
    "\n",
    "model.recommend(user_id, users_items_pivot_sparse_matrix, N=30)\n",
    "\n",
    "### Generating recommendations for News_id / item_id\n",
    "\n",
    "item_id = 20\n",
    "n_similar = 2982\n",
    "\n",
    "similar = model.similar_items(item_id, n_similar)\n",
    "\n",
    "similar\n",
    "\n",
    "output = pd.DataFrame(similar, columns=['news_id', 'als_score'])\n",
    "\n",
    "output\n",
    "\n",
    "## Merging recommendations\n",
    "\n",
    "consumer_ds1 = consumer_ds[['consumer_id', 'item_id', 'interaction_type']]\n",
    "\n",
    "# Consusmer Interaction Type\n",
    "interaction = consumer_ds1['interaction_type'].value_counts()*100/consumer_ds1.shape[0]\n",
    "\n",
    "interaction\n",
    "\n",
    "interaction_ds = pd.DataFrame(interaction)\n",
    "\n",
    "type(interaction_ds)\n",
    "\n",
    "interaction_ds\n",
    "\n",
    "interaction_ds.reset_index(inplace=True)\n",
    "\n",
    "interaction_ds\n",
    "\n",
    "interaction_ds.columns = ['interaction_type', 'value']\n",
    "\n",
    "### Creating a rating column\n",
    "\n",
    "interaction_ds['rating1'] = 100/interaction_ds['value']\n",
    "\n",
    "interaction_ds.head(3)\n",
    "\n",
    "consumer_ratings = pd.merge(consumer_ds1, interaction_ds, on='interaction_type', how='left')\n",
    "\n",
    "consumer_ratings.head(3)\n",
    "\n",
    "consumer_ratings = consumer_ratings[['consumer_id', 'item_id', 'rating1']]\n",
    "\n",
    "consumer_ratings.head(3)\n",
    "\n",
    "consumer_ratings.shape\n",
    "\n",
    "consumer_ratings['news_id'] = consumer_ratings.groupby(['item_id']).ngroup()\n",
    "\n",
    "consumer_ratings['user_id'] = consumer_ratings.groupby(['consumer_id']).ngroup()\n",
    "\n",
    "consumer_ratings.head(3)\n",
    "\n",
    "consumer_ratings.describe()\n",
    "\n",
    "ratings_ds = consumer_ratings[['user_id', 'news_id', 'rating1']]\n",
    "\n",
    "ratings_ds.head(3)\n",
    "\n",
    "news_mapping = consumer_ratings[['item_id', 'news_id']]\n",
    "\n",
    "news_mapping.head(3)\n",
    "\n",
    "news_mapping.shape\n",
    "\n",
    "news_mapping1 = news_mapping.drop_duplicates()\n",
    "\n",
    "news_mapping1.shape\n",
    "\n",
    "als_ds1 = pd.merge(output, news_mapping1, on='news_id', how='left')\n",
    "\n",
    "als_ds1.head()\n",
    "\n",
    "news_title.shape\n",
    "\n",
    "als_title = pd.merge(als_ds1, news_title, on='item_id', how='left')\n",
    "\n",
    "als_title.head(3)\n",
    "\n",
    "als_title.shape\n",
    "\n",
    "# Calculating ALS Score using Normalization\n",
    "\n",
    "als_title['als_score_normalized'] = (als_title['als_score']-min(als_title['als_score'])) / (max(als_title['als_score']) - min(als_title['als_score']))\n",
    "\n",
    "als_title\n",
    "\n",
    "\n",
    "\n",
    "# Hybrid Recommendation System\n",
    "\n",
    "## Hybrid-1: Content Based + Item Based Collaboriative Model\n",
    "\n",
    "news_index = 30\n",
    "\n",
    "news_prediction = pd.DataFrame(item_correlation)\n",
    "\n",
    "news_prediction.head(3)\n",
    "\n",
    "item_recommendation = pd.DataFrame(news_prediction.iloc[news_index].sort_values(ascending=False))\n",
    "\n",
    "item_recommendation\n",
    "\n",
    "item_recommendation.reset_index(inplace=True)\n",
    "\n",
    "item_recommendation.columns = ['news_id', 'score']\n",
    "item_recommendation.head(3)\n",
    "\n",
    "### Merging\n",
    "\n",
    "merged_ds1 = pd.merge(item_recommendation, news_mapping1, on='news_id', how='left')\n",
    "\n",
    "merged_ds1.head(3)\n",
    "\n",
    "hybrid1 = pd.merge(merged_ds1, platform_ds3, on='item_id', how='left')\n",
    "\n",
    "hybrid1.shape\n",
    "\n",
    "hybrid1 = hybrid1[['news_id','item_id','title','score']]\n",
    "\n",
    "hybrid1.head(3)\n",
    "\n",
    "hybrid1['collaborative_score_normalized'] = (hybrid1['score']-min(hybrid1['score']))/(max(hybrid1['score'])-min(hybrid1['score']))\n",
    "\n",
    "hybrid1.head(3)\n",
    "\n",
    "news_recom = news_recommendation('Machine Learning for Designers', 10)\n",
    "\n",
    "news_ds = pd.DataFrame(news_recom)\n",
    "\n",
    "news_ds.head(3)\n",
    "\n",
    "news_ds.reset_index(inplace=True)\n",
    "\n",
    "news_ds.columns = ['title', 'score']\n",
    "\n",
    "news_ds.head(3)\n",
    "\n",
    "news_ds['content_score_normalized'] = (news_ds['score']-min(news_ds['score'])) / (max(news_ds['score'])-min(news_ds['score']))\n",
    "\n",
    "news_ds\n",
    "\n",
    "\n",
    "\n",
    "hybrid1_result = pd.merge(hybrid1, news_ds, how='left', on='title')\n",
    "\n",
    "hybrid1_result.head(3)\n",
    "\n",
    "\n",
    "\n",
    "hybrid1_result['Main_Score'] = (hybrid1_result['collaborative_score_normalized']+hybrid1_result['content_score_normalized'])/2\n",
    "\n",
    "hybrid1_result = hybrid1_result[['title','Main_Score']]\n",
    "\n",
    "hybrid1_result.shape\n",
    "\n",
    "hybrid1_result = hybrid1_result.drop_duplicates()\n",
    "\n",
    "hybrid1_result.sort_values(by='Main_Score', ascending=False)[:10]\n",
    "\n",
    "\n",
    "\n",
    "## Hybrid-2: ALS + Item-based Collaborative Filtering\n",
    "\n",
    "hybrid2 = pd.merge(hybrid1, als_title, how='left', on='title')\n",
    "\n",
    "hybrid2.head(3)\n",
    "\n",
    "hybrid2.shape\n",
    "\n",
    "hybrid2['Main_Score'] = (hybrid2['collaborative_score_normalized']+hybrid2['als_score_normalized'])/2\n",
    "\n",
    "hybrid2.head(3)\n",
    "\n",
    "hybrid2 = hybrid2[['title','Main_Score']]\n",
    "\n",
    "hybrid2_result = hybrid2.drop_duplicates()\n",
    "\n",
    "hybrid2_result.sort_values(by='Main_Score', ascending=False)[:10]\n",
    "\n",
    "\n",
    "\n",
    "# Eavluation Metrics & Conclusion\n",
    "\n",
    "Main Evaluation Metrics used here are:\n",
    "\n",
    "#### RMSE\n",
    "Root Mean Square Error is the standard deviation of the residuals (prediction errors). It serves to aggregate the magnitudes of the errors in predictions for various data points into a single measure of predictive power. RMSE is a measure of accuracy, to compare forecasting errors of different models for a particular dataset and not between datasets, as it is scale-dependent. It is extremely helpful to have a single number to judge a model’s performance. RMSE is a good choice as it is used mainly used for analysis & forecasting.\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Root mean square error can be expressed as above, where N is the number of data points, y(i) is the i-th measurement, and y ̂^(i) is its corresponding prediction.\n",
    "\n",
    "\n",
    "#### Precision@k\n",
    "Precision@k is the proportion of recommended items in the top-k set that are relevant.\n",
    "We can say that,\n",
    "\n",
    "Precision@k = (No. of recommended items @k that are relevant) / (No. of recommended items @k)\n",
    "\n",
    "\n",
    "For instance, Precision@10 calculated here, corresponds to the number of relevant results among the top 10 retrieved recommendations. This metric is chosen as it is easier to score manually since only the top k results need to be examined to determine if they are relevant or not.\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Above formula for understanding precision@k in top-k set.\n",
    "\n",
    "There are various other evaluation models including MAE, MAP@k, recall@k and so on that could also bring some insights to the receommendation model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
